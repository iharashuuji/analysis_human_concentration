# World Modelを用いたリアルタイム集中度分析

本プロジェクトは、「ワールドモデル」の概念に基づいた深層学習モデルを使用し、人間の集中度をリアルタイムで分析するデモンストレーションです。Webカメラから映像を取得し、ユーザーの顔を分析してリアルタイムで「集中度スコア」を算出します。スコアが高いほど、集中度が低い（逸脱している）ことを示します。

このシステムはPython、PyTorch、Streamlitで構築されており、セットアップとデプロイを容易にするためにDockerコンテナ内で実行されるように設計されています。

![Application Screenshot](https://user-images.githubusercontent.com/your-github-id/your-repo/your-image.png)
*(ここにアプリケーションのスクリーンショットを追加することを推奨します)*

## 目次
1.  [コンセプトと技術的背景](#コンセプトと技術的背景)
    *   [ワールドモデル・アーキテクチャ](#ワールドモデルアーキテクチャ)
    *   [β-VAEによるDisentangled（分離）表現学習](#β-vaeによるdisentangled分離表現学習)
    *   [MDN-RNNによる確率的予測](#mdn-rnnによる確率的予測)
2.  [主な機能](#主な機能)
3.  [システム構成](#システム構成)
4.  [セットアップと使い方](#セットアップと使い方)
    *   [前提条件](#前提条件)
    *   [モデルの訓練](#モデルの訓練)
    *   [アプリケーションの実行](#アプリケーションの実行)
5.  [ファイル構成](#ファイル構成)
6.  [プルリクエスト：最近の改善点の概要](#プルリクエスト最近の改善点の概要)

---

## コンセプトと技術的背景

このプロジェクトの中核は、Google Brainの研究者らによって広められた概念である「ワールドモデル」です。ワールドモデルは、環境の空間的・時間的な情報を圧縮した表現を学習します。このプロジェクトでは、ユーザーの顔のビデオフィードが「世界」にあたります。モデルは「正常な（集中している）」顔の振る舞いがどのようなものかを学習し、そこからの逸脱を異常として検出します。

これは、2つの部分からなるアーキテクチャによって実現されます。

### 1. ワールドモデル・アーキテクチャ

私たちのモデルは、主に2つのコンポーネントで構成されています。

*   **変分オートエンコーダ (VAE)**: これは**観測モデル (V)**です。高次元の入力（64x64の顔画像）を受け取り、それを低次元の潜在ベクトル`z`に圧縮します。このベクトルは、顔の重要な特徴を圧縮された形で捉えます。VAEは、この潜在ベクトルから元の画像を再構成するように訓練されるため、`z`は最も重要な情報を保持せざるを得なくなります。

*   **再帰型ニューラルネットワーク (RNN)**: これは**ダイナミクスモデル (M)**です。潜在ベクトル`z`の時系列的なパターンを学習します。顔の現在の状態（`z_t`で表現）が与えられると、RNNは次にありそうな状態（`z_t+1`）を予測します。

**異常検知の仕組み:** 「集中度スコア」（異常スコア）は、RNNの予測と顔の実際の状態との差に基づいて計算されます。ユーザーが予期しない動き（あくびやよそ見など）をすると、実際の`z`ベクトルはRNNの予測と大きく異なるため、高い異常スコアが算出されます。

### 2. β-VAEによるDisentangled（分離）表現学習

潜在空間の質を向上させるため、**β-VAE**フレームワークを導入しました。

*   **参照論文:** Higgins et al., "β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework" (ICLR 2017)
*   **改善点:** 標準的なVAEの損失関数は `再構成誤差 + KLダイバージェンス` です。β-VAEはこれを `再構成誤差 + β * KLダイバージェンス` に変更します。`β > 1`（本プロジェクトでは`β=4.0`を使用）に設定することで、潜在空間に強い制約を課します。これにより、モデルはより**Disentangled（分離された）な表現**を学習するよう促されます。これは、潜在ベクトル`z`の各次元が、顔の独立した変動要因（例：頭の向き、目の開閉、口の形など）に対応するようになることを意味します。
*   **利点:** Disentangledな潜在空間は、後続のRNN（ダイナミクスモデル）にとって非常に有益です。RNNは、これらのクリーンで独立した要素の時間的なダイナミクスをより簡単にモデル化でき、結果として「次の状態」の予測がより堅牢になります。これにより、予測と実際の状態の差である異常スコアが、より敏感で意味のあるものになります。例えば、「口の形」の次元での急な変化（あくび）は、複雑に絡み合った表現の変化よりも明確な異常信号となります。

### 3. MDN-RNNによる確率的予測

ダイナミクスモデルには、**混合密度ネットワークRNN (MDN-RNN)** を使用しています。

*   **参照論文:** Graves, "Generating Sequences With Recurrent Neural Networks" (2013)
*   **改善点:** 標準的なRNNは単一の決定論的な次の状態を予測します。対照的に、MDN-RNNは次の状態の**確率分布**（具体的には混合ガウスモデル - GMM）を予測します。これは、不確実性をモデル化できるため、より強力です。
*   **利点:** 異常スコアは、予測された分布における実際の次の状態の負の対数尤度（NLL）を用いて計算されます。観測された状態が、モデルが学習したダイナミクスによれば非常にあり得ないものである場合、NLLは非常に高くなり、堅牢な異常信号を提供します。このアプローチは、単純な平均二乗誤差（MSE）よりも原理に基づいた予測誤差の測定方法です。

最終的な異常スコアは、以下の2つの要素の加重和です。
1.  **予測誤差 (NLL):** MDN-RNNの時間的予測からの誤差。
2.  **再構成誤差 (MSE):** VAEの画像再構成からの誤差。これは、VAEがうまく再構成できない視覚的な異常（例：奇妙な表情）を検出するのに役立ちます。

## 主な機能

*   **リアルタイム分析:** Webカメラの映像をリアルタイムで分析し、集中度スコアを計算します。
*   **動画ファイル分析:** 録画済みの動画ファイルをアップロードして分析します。
*   **視覚的フィードバック:** 検出された顔と現在の異常スコアをビデオフィード上に描画します。
*   **インタラクティブなグラフ:** 集中度スコアを時系列でプロットし、簡単に監視できます。
*   **状態リセット:** 新しいセッションを分析するために、RNNの内部状態をリセットできます。

## システム構成

アプリケーションはDockerコンテナ内で実行され、すべての依存関係が正しく管理されます。

1.  **フロントエンド:** Streamlitウェブアプリケーション（`app_realtime.py`, `pages/1_Analyze_Video.py`）がユーザーインターフェースを提供します。
2.  **バックエンド映像処理:** `streamlit-webrtc`が、クライアントのブラウザからサーバーへのリアルタイムビデオストリーミングを処理します。
3.  **コアロジック:** `RealtimeProcessor`クラス（`utils/processor.py`）が、すべての深層学習ロジックをカプセル化します。
    *   OpenCVのHaar Cascade分類器による顔検出。
    *   画像の前処理。
    *   PyTorchを使用したVAEおよびMDN-RNNモデルによる推論。
    *   最終的な異常スコアの計算。
4.  **モデル:** 事前学習済みのVAEおよびMDN-RNNモデルが`weights/`ディレクトリからロードされます。

## セットアップと使い方

### 前提条件

*   Docker
*   Docker Compose（通常はDocker Desktopに含まれています）
*   CUDAドライバがインストールされたNVIDIA GPU（GPUアクセラレーション用）。

### モデルの訓練

アプリケーションを実行する前に、VAEとMDN-RNNモデルを訓練する必要があります。

1.  **データ準備:** 訓練用の動画（「集中している」状態の人物が映っているもの）を`/data`ディレクトリに配置します。

2.  **Dockerイメージのビルド:**
    ```bash
    docker compose build
    ```

3.  **訓練スクリプトの実行:**
    Dockerコンテナ内で訓練スクリプトを実行します。これにより、まずVAEが訓練され、次にそのVAEを使用してMDN-RNNが訓練され、重みが`weights/`ディレクトリに保存されます。
    ```bash
    docker compose run --rm app python world_model_train.py
    ```
    *注意: `--rm`フラグは、スクリプト終了後にコンテナを自動的に削除します。*

### アプリケーションの実行

モデルが訓練され、`weights/`ディレクトリに`vae_engage3_only.pth`と`rnn_engage3_only.pth`が含まれていることを確認したら、Streamlitアプリケーションを起動できます。

1.  **サーバーの起動:**
    ```bash
    docker compose up -d
    ```
    `-d`フラグは、コンテナをデタッチモードで実行します。

2.  **アプリケーションへのアクセス:**
    ウェブブラウザを開き、`http://localhost:8501`にアクセスします。

3.  **サーバーの停止:**
    ```bash
    docker compose down
    ```

## ファイル構成
```

. ├── Dockerfile 
  ├── docker-compose.yml 
  ├── requirements.txt 
  ├── app_realtime.py # リアルタイムデモのメインスクリプト 
  ├── world_model_train.py # VAEとRNNモデルの訓練用スクリプト 
  ├── pages/ 
          │ └── 1_Analyze_Video.py # 動画ファイル分析用のStreamlitページ 
  ├── models/ 
          │ └── world_model.py # VAEとRNNモデルの定義 
  ├── utils/ │ 
          ├── loader.py # モデルをロードするためのユーティリティ │ 
          ├── logger.py # ロガーの設定 
          │ └── processor.py # フレーム処理と異常スコアリングのコアロジック 
  ├── weights/ # 訓練済みモデルの重み用ディレクトリ（訓練後に作成） │ 
          ├── vae_engage3_only.pth 
          │ └── rnn_engage3_only.pth 
  └── data/ # 訓練用動画のディレクトリ └── (your_videos.mp4)

```

## プルリクエスト：最近の改善点の概要

### 機能：β-VAEとMDN-RNNによる訓練の強化

このアップデートは、先進的で研究に裏付けられた技術を取り入れることにより、中核となる訓練および推論ロジックを大幅に改善するものです。目的は、集中度分析のためのより堅牢で高感度なモデルを作成することです。

**変更点:**

1.  **VAE訓練へのβ-VAEの導入:**
    *   **ファイル:** `world_model_train.py`
    *   **理由:** 論文 **"β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"** に基づき、VAEの損失関数に`β`パラメータを導入しました。
    *   **改善点:** `β > 1`と設定することで、VAEが**Disentangled（分離された）な潜在空間**を学習するように促します。これにより、モデルは顔の独立した特徴（頭の向き、目の状態など）を潜在ベクトルの異なる次元に沿って表現するようになり、表現がより構造化され解釈しやすくなります。これは、後続のダイナミクスモデルにとって、よりクリーンな信号を提供します。

2.  **ダイナミクスモデルへのMDN-RNNの採用:**
    *   **ファイル:** `world_model_train.py`, `models/world_model.py`, `utils/loader.py`, `utils/processor.py`
    *   **理由:** **"Generating Sequences With Recurrent Neural Networks"** の原則に従い、単純なMSEベースのRNNを**混合密度ネットワークRNN (MDN-RNN)** に置き換えました。
    *   **改善点:** 単一の次の状態を予測する代わりに、MDN-RNNは可能な次の状態の**確率分布**（混合ガウスモデル）を予測します。異常スコアは、この分布における実際に観測された状態の負の対数尤度（NLL）となりました。これは、モデルの予測における不確実性を考慮するため、「驚き」や異常を測定するためのより原理的な方法です。

3.  **異常スコア計算の洗練:**
    *   **ファイル:** `utils/processor.py`
    *   **改善点:** 最終的な異常スコアは、2つの異なる誤差信号の加重和になりました。
        1.  **予測誤差 (NLL):** 時間的な異常（予期しない動き）を捉えます。
        2.  **再構成誤差 (MSE):** VAEが再構成に苦労する視覚的な異常（珍しい表情）を捉えます。
    *   この二重ソースのスコアは、ユーザーの集中度の欠如をより包括的かつ信頼性高く測定します。

これらの変更により、本プロジェクトは単純なワールドモデルの実装から、確立された深
